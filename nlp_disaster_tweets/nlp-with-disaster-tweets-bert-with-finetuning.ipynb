{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-02-11T08:12:41.131069Z",
     "iopub.status.busy": "2023-02-11T08:12:41.130576Z",
     "iopub.status.idle": "2023-02-11T08:12:41.166916Z",
     "shell.execute_reply": "2023-02-11T08:12:41.165857Z",
     "shell.execute_reply.started": "2023-02-11T08:12:41.131027Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T08:12:41.174139Z",
     "iopub.status.busy": "2023-02-11T08:12:41.169583Z",
     "iopub.status.idle": "2023-02-11T08:13:05.264521Z",
     "shell.execute_reply": "2023-02-11T08:13:05.263331Z",
     "shell.execute_reply.started": "2023-02-11T08:12:41.174101Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jethr\\anaconda3\\envs\\ml\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\jethr\\anaconda3\\envs\\ml\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\jethr\\anaconda3\\envs\\ml\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\jethr\\anaconda3\\envs\\ml\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\jethr\\anaconda3\\envs\\ml\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "#import gc\n",
    "import re\n",
    "import string\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn import metrics\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "SEED = 1337\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline,BertTokenizer \n",
    "from bs4 import BeautifulSoup\n",
    "!pip install contractions\n",
    "import contractions\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T08:13:05.268008Z",
     "iopub.status.busy": "2023-02-11T08:13:05.266677Z",
     "iopub.status.idle": "2023-02-11T08:13:05.278074Z",
     "shell.execute_reply": "2023-02-11T08:13:05.275874Z",
     "shell.execute_reply.started": "2023-02-11T08:13:05.267954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jethr\\OneDrive\\Documents\\Kaggle\n",
      "C:\\Users\\jethr\\OneDrive\\Documents\\Kaggle\\input\n"
     ]
    }
   ],
   "source": [
    "PARENT_DIR = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "INPUT_DIR = os.path.join(PARENT_DIR,\"data\")\n",
    "TRAIN_CSV = os.path.join(INPUT_DIR,\"train.csv\")\n",
    "TEST_CSV = os.path.join(INPUT_DIR,\"test.csv\")\n",
    "print(PARENT_DIR)\n",
    "print(INPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T08:14:41.119934Z",
     "iopub.status.busy": "2023-02-11T08:14:41.118900Z",
     "iopub.status.idle": "2023-02-11T08:14:41.162384Z",
     "shell.execute_reply": "2023-02-11T08:14:41.161238Z",
     "shell.execute_reply.started": "2023-02-11T08:14:41.119886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape = (7613, 5)\n",
      "Training Set Memory Usage = 0.20 MB\n",
      "Test Set Shape = (3263, 4)\n",
      "Test Set Memory Usage = 0.08 MB\n"
     ]
    }
   ],
   "source": [
    "train_csv = pd.read_csv(TRAIN_CSV, dtype={'id': np.int16, 'target': np.int8})\n",
    "test_csv = pd.read_csv(TEST_CSV, dtype={'id': np.int16})\n",
    "df_train = pd.DataFrame(train_csv)\n",
    "df_test = pd.DataFrame(test_csv)\n",
    "\n",
    "\n",
    "print('Training Set Shape = {}'.format(df_train.shape))\n",
    "print('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\n",
    "print('Test Set Shape = {}'.format(df_test.shape))\n",
    "print('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T08:14:42.884664Z",
     "iopub.status.busy": "2023-02-11T08:14:42.884113Z",
     "iopub.status.idle": "2023-02-11T08:14:43.872316Z",
     "shell.execute_reply": "2023-02-11T08:14:43.871318Z",
     "shell.execute_reply.started": "2023-02-11T08:14:42.884629Z"
    }
   },
   "outputs": [],
   "source": [
    "#understanding metadata\n",
    "df_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\n",
    "df_test['word_count'] = df_test['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# unique_word_count\n",
    "df_train['unique_word_count'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\n",
    "df_test['unique_word_count'] = df_test['text'].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "# stop_word_count\n",
    "df_train['stop_word_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "df_test['stop_word_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "\n",
    "# url_count\n",
    "df_train['url_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "df_test['url_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "\n",
    "# mean_word_length\n",
    "df_train['mean_word_length'] = df_train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_test['mean_word_length'] = df_test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "# char_count\n",
    "df_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\n",
    "df_test['char_count'] = df_test['text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# punctuation_count\n",
    "df_train['punctuation_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df_test['punctuation_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "# hashtag_count\n",
    "df_train['hashtag_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "df_test['hashtag_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "\n",
    "# mention_count\n",
    "df_train['mention_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
    "df_test['mention_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
    "df_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\n",
    "df_test['word_count'] = df_test['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# unique_word_count\n",
    "df_train['unique_word_count'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\n",
    "df_test['unique_word_count'] = df_test['text'].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "# stop_word_count\n",
    "df_train['stop_word_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "df_test['stop_word_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "\n",
    "# url_count\n",
    "df_train['url_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "df_test['url_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "\n",
    "# mean_word_length\n",
    "df_train['mean_word_length'] = df_train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_test['mean_word_length'] = df_test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "# char_count\n",
    "df_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\n",
    "df_test['char_count'] = df_test['text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# punctuation_count\n",
    "df_train['punctuation_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df_test['punctuation_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "# hashtag_count\n",
    "df_train['hashtag_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "df_test['hashtag_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "\n",
    "# mention_count\n",
    "df_train['mention_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
    "df_test['mention_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referring to [Gunes Evitan's Notebook](https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert) and [Rohit Gurad's Notebook](https://www.kaggle.com/code/rohitgarud/all-almost-data-preprocessing-techniques-for-nlp) on data cleaning & practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T08:14:43.936703Z",
     "iopub.status.busy": "2023-02-11T08:14:43.936308Z",
     "iopub.status.idle": "2023-02-11T08:14:43.943517Z",
     "shell.execute_reply": "2023-02-11T08:14:43.942181Z",
     "shell.execute_reply.started": "2023-02-11T08:14:43.936670Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jethr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jethr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Set-up Lemmatization\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T08:14:44.095637Z",
     "iopub.status.busy": "2023-02-11T08:14:44.094934Z",
     "iopub.status.idle": "2023-02-11T08:14:59.869248Z",
     "shell.execute_reply": "2023-02-11T08:14:59.868029Z",
     "shell.execute_reply.started": "2023-02-11T08:14:44.095600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: symspellpy in c:\\users\\jethr\\anaconda3\\envs\\ml\\lib\\site-packages (6.7.7)\n",
      "Requirement already satisfied: editdistpy>=0.1.3 in c:\\users\\jethr\\anaconda3\\envs\\ml\\lib\\site-packages (from symspellpy) (0.1.3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set up word correction\n",
    "!pip install symspellpy\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\"\n",
    ")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T08:14:59.871970Z",
     "iopub.status.busy": "2023-02-11T08:14:59.871537Z",
     "iopub.status.idle": "2023-02-11T08:14:59.879549Z",
     "shell.execute_reply": "2023-02-11T08:14:59.878531Z",
     "shell.execute_reply.started": "2023-02-11T08:14:59.871919Z"
    }
   },
   "outputs": [],
   "source": [
    "def correct_spelling_symspell(text):\n",
    "    words = [\n",
    "        sym_spell.lookup(\n",
    "            word, \n",
    "            Verbosity.CLOSEST, \n",
    "            max_edit_distance=2,\n",
    "            include_unknown=True\n",
    "            )[0].term \n",
    "        for word in text.split()] \n",
    "    text = \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T08:14:59.881698Z",
     "iopub.status.busy": "2023-02-11T08:14:59.881163Z",
     "iopub.status.idle": "2023-02-11T08:15:00.237011Z",
     "shell.execute_reply": "2023-02-11T08:15:00.236124Z",
     "shell.execute_reply.started": "2023-02-11T08:14:59.881653Z"
    }
   },
   "outputs": [],
   "source": [
    "#correctin compouund words\n",
    "bigram_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\"\n",
    ")\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "\n",
    "def correct_spelling_symspell_compound(text):\n",
    "    words = [\n",
    "        sym_spell.lookup_compound(\n",
    "            word, \n",
    "            max_edit_distance=2\n",
    "            )[0].term \n",
    "        for word in text.split()] \n",
    "    text = \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T08:15:00.240209Z",
     "iopub.status.busy": "2023-02-11T08:15:00.239807Z",
     "iopub.status.idle": "2023-02-11T08:15:00.253123Z",
     "shell.execute_reply": "2023-02-11T08:15:00.251947Z",
     "shell.execute_reply.started": "2023-02-11T08:15:00.240173Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    #fill NA\n",
    "    data[\"keyword\"] = data[\"keyword\"].fillna(\"\")\n",
    "    data[\"location\"] = data[\"location\"].fillna(\"\")\n",
    "    #combine keyword with text\n",
    "    data[\"tweet\"] = data[\"keyword\"] + \" \" + data[\"text\"]\n",
    "    #lower case\n",
    "    data[\"tweet\"] = data[\"tweet\"].str.lower()\n",
    "    #remove HTML\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(lambda x: BeautifulSoup(x).get_text())\n",
    "    #expand contractions\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(contractions.fix)\n",
    "    #remove URLs\n",
    "    url_pattern = re.compile(r'https?://(www\\.)?(\\w+)(\\.\\w+)(/\\w*)?')\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(lambda x: re.sub(url_pattern,\"\",x))\n",
    "    #remove emails\n",
    "    email_pattern = re.compile(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\")\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(lambda x: re.sub(email_pattern,\"\",x))\n",
    "    #remove tweet mentions\n",
    "    mention_pattern = re.compile(r\"@\\w+\")\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(lambda x: re.sub(mention_pattern,\"\",x))\n",
    "    #remove punctuations, keeping hashtags to tokenize\n",
    "    punc_clean = string.punctuation.replace(\"#\",\"\")\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(lambda x: re.sub('[%s]' % re.escape(punc_clean), \" \",x))\n",
    "    #remove digits\n",
    "    digit_pattern = re.compile(\"\\w*\\d+\\w*\")\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(lambda x: re.sub(digit_pattern,\"\",x))  \n",
    "    #remove extra space\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(lambda x: re.sub(' +', ' ', x).strip())\n",
    "    #lemmatize\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(lemmatize_text)\n",
    "    #clean spelling correction\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(correct_spelling_symspell)\n",
    "    #remove compound\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(correct_spelling_symspell_compound)\n",
    "    #remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(lambda x: \" \".join([word for word in str(x).split() if word not in stop_words]))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T08:15:00.255259Z",
     "iopub.status.busy": "2023-02-11T08:15:00.254528Z",
     "iopub.status.idle": "2023-02-11T08:15:17.814492Z",
     "shell.execute_reply": "2023-02-11T08:15:17.813502Z",
     "shell.execute_reply.started": "2023-02-11T08:15:00.255223Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jethr\\anaconda3\\envs\\ml\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jethr\\anaconda3\\envs\\ml\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#cleaning data\n",
    "df_train = clean_data(df_train)\n",
    "df_test = clean_data(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T08:22:27.059627Z",
     "iopub.status.busy": "2023-02-11T08:22:27.058858Z",
     "iopub.status.idle": "2023-02-11T08:22:27.073676Z",
     "shell.execute_reply": "2023-02-11T08:22:27.071413Z",
     "shell.execute_reply.started": "2023-02-11T08:22:27.059584Z"
    }
   },
   "outputs": [],
   "source": [
    "#Creating CustomDataset Class\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len,data_type):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.tweet = dataframe.tweet\n",
    "        self.word_count = dataframe.word_count\n",
    "        self.unique_word_count = dataframe.unique_word_count\n",
    "        self.stop_word_count = dataframe.stop_word_count\n",
    "        self.mean_word_length = dataframe.mean_word_length\n",
    "        self.char_count = dataframe.char_count\n",
    "        self.punctuation_count = dataframe.punctuation_count\n",
    "        self.max_len = max_len\n",
    "        self.data_type = data_type\n",
    "        \n",
    "        if data_type == \"train\":\n",
    "            self.targets = dataframe.target\n",
    "            self.data_type = data_type\n",
    "        else:\n",
    "            self.targets = \"\"\n",
    "            self.data_type = \"test\"\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tweet = str(self.tweet[index])\n",
    "        tweet = \" \".join(tweet.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            tweet,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        if self.data_type == \"train\":\n",
    "            return {\n",
    "                'ids': torch.tensor(ids, dtype=torch.long),\n",
    "                'mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'ids': torch.tensor(ids, dtype=torch.long),\n",
    "                'mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T08:16:39.875699Z",
     "iopub.status.busy": "2023-02-11T08:16:39.875267Z",
     "iopub.status.idle": "2023-02-11T08:16:44.088849Z",
     "shell.execute_reply": "2023-02-11T08:16:44.087451Z",
     "shell.execute_reply.started": "2023-02-11T08:16:39.875651Z"
    }
   },
   "outputs": [],
   "source": [
    "#load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T08:23:47.898046Z",
     "iopub.status.busy": "2023-02-11T08:23:47.897536Z",
     "iopub.status.idle": "2023-02-11T08:23:47.908382Z",
     "shell.execute_reply": "2023-02-11T08:23:47.906860Z",
     "shell.execute_reply.started": "2023-02-11T08:23:47.898004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (7613, 15)\n",
      "TRAIN Dataset: (6090, 15)\n",
      "TEST Dataset: (1523, 15)\n"
     ]
    }
   ],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 100\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-05\n",
    "\n",
    "# Creating the dataset and dataloader for the neural network\n",
    "train_size = 0.8\n",
    "train_dataset=df_train.sample(frac=train_size,random_state=200)\n",
    "dev_dataset=df_train.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df_train.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(dev_dataset.shape))\n",
    "\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN,\"train\")\n",
    "testing_set = CustomDataset(dev_dataset, tokenizer, MAX_LEN,\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T08:23:48.627072Z",
     "iopub.status.busy": "2023-02-11T08:23:48.626614Z",
     "iopub.status.idle": "2023-02-11T08:23:48.633551Z",
     "shell.execute_reply": "2023-02-11T08:23:48.632237Z",
     "shell.execute_reply.started": "2023-02-11T08:23:48.627038Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load in batch parameters\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T08:23:49.185695Z",
     "iopub.status.busy": "2023-02-11T08:23:49.185023Z",
     "iopub.status.idle": "2023-02-11T08:23:51.806387Z",
     "shell.execute_reply": "2023-02-11T08:23:51.805243Z",
     "shell.execute_reply.started": "2023-02-11T08:23:49.185657Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\jethr\\AppData\\Local\\Temp\\ipykernel_24976\\2462764309.py:13: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(self.l3.weight)\n",
      "C:\\Users\\jethr\\AppData\\Local\\Temp\\ipykernel_24976\\2462764309.py:14: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(self.l4.weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l2): Dropout(p=0.3, inplace=False)\n",
       "  (l3): Linear(in_features=768, out_features=30, bias=True)\n",
       "  (l4): Linear(in_features=30, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased',return_dict=False)\n",
    "        #self.l1 = BertForSequenceClassification.from_pretrained('bert-large-uncased',num_labels = 2,output_attentions = False,output_hidden_states = False)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 30)\n",
    "        self.l4 = torch.nn.Linear(30,1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        torch.nn.init.xavier_uniform(self.l3.weight)\n",
    "        torch.nn.init.xavier_uniform(self.l4.weight)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output_3 = self.l3(output_2)\n",
    "        output_4 = self.relu(output_3)\n",
    "        output_s = self.l4(output_4)\n",
    "        output = self.sigmoid(output_s)\n",
    "        return output\n",
    "    \n",
    "\n",
    "model = BERTClass()\n",
    "model.to(torch.device('cuda:0')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T08:21:37.042434Z",
     "iopub.status.busy": "2023-02-11T08:21:37.042016Z",
     "iopub.status.idle": "2023-02-11T08:21:37.047647Z",
     "shell.execute_reply": "2023-02-11T08:21:37.046597Z",
     "shell.execute_reply.started": "2023-02-11T08:21:37.042395Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T08:21:37.050181Z",
     "iopub.status.busy": "2023-02-11T08:21:37.049663Z",
     "iopub.status.idle": "2023-02-11T08:21:37.058700Z",
     "shell.execute_reply": "2023-02-11T08:21:37.057606Z",
     "shell.execute_reply.started": "2023-02-11T08:21:37.050128Z"
    }
   },
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "#optimizer = torch.optim.SGD(params =  model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = LEARNING_RATE, # args.learning_rate\n",
    "                  eps = 1e-8 # args.adam_epsilon\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "train_history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T08:24:45.292141Z",
     "iopub.status.busy": "2023-02-11T08:24:45.291663Z",
     "iopub.status.idle": "2023-02-11T08:24:45.303843Z",
     "shell.execute_reply": "2023-02-11T08:24:45.301997Z",
     "shell.execute_reply.started": "2023-02-11T08:24:45.292100Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    batch = 0\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = loss_fn(outputs[:,0], targets)\n",
    "        if _%10==0:\n",
    "            batch +=1\n",
    "            train_history[batch] = loss.item()\n",
    "            train_history[epoch] = loss.item()\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1764871680"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T08:24:45.497833Z",
     "iopub.status.busy": "2023-02-11T08:24:45.496823Z",
     "iopub.status.idle": "2023-02-11T08:30:46.747772Z",
     "shell.execute_reply": "2023-02-11T08:30:46.746239Z",
     "shell.execute_reply.started": "2023-02-11T08:24:45.497797Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jethr\\anaconda3\\envs\\ml\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.7203698754310608\n",
      "Epoch: 0, Loss:  0.8127346038818359\n",
      "Epoch: 0, Loss:  0.7164047956466675\n",
      "Epoch: 0, Loss:  0.7067713141441345\n",
      "Epoch: 0, Loss:  0.687717080116272\n",
      "Epoch: 0, Loss:  0.7595506906509399\n",
      "Epoch: 0, Loss:  0.5766982436180115\n",
      "Epoch: 0, Loss:  0.6021121740341187\n",
      "Epoch: 0, Loss:  0.6524345278739929\n",
      "Epoch: 0, Loss:  0.7068862915039062\n",
      "Epoch: 0, Loss:  0.668786883354187\n",
      "Epoch: 0, Loss:  0.6812465190887451\n",
      "Epoch: 0, Loss:  0.6420034170150757\n",
      "Epoch: 0, Loss:  0.5425136089324951\n",
      "Epoch: 0, Loss:  0.5363538861274719\n",
      "Epoch: 0, Loss:  0.7015774250030518\n",
      "Epoch: 0, Loss:  0.6270390152931213\n",
      "Epoch: 0, Loss:  0.6577683687210083\n",
      "Epoch: 0, Loss:  0.6740067005157471\n",
      "Epoch: 0, Loss:  0.7094087600708008\n",
      "Epoch: 0, Loss:  0.6855168342590332\n",
      "Epoch: 0, Loss:  0.5982848405838013\n",
      "Epoch: 0, Loss:  0.5922035574913025\n",
      "Epoch: 0, Loss:  0.597419261932373\n",
      "Epoch: 0, Loss:  0.618679404258728\n",
      "Epoch: 0, Loss:  0.5827774405479431\n",
      "Epoch: 0, Loss:  0.6250474452972412\n",
      "Epoch: 0, Loss:  0.6017131209373474\n",
      "Epoch: 0, Loss:  0.6370781660079956\n",
      "Epoch: 0, Loss:  0.5835245847702026\n",
      "Epoch: 0, Loss:  0.6521816253662109\n",
      "Epoch: 0, Loss:  0.6207959651947021\n",
      "Epoch: 0, Loss:  0.5745658278465271\n",
      "Epoch: 0, Loss:  0.5298382639884949\n",
      "Epoch: 0, Loss:  0.7077816724777222\n",
      "Epoch: 0, Loss:  0.5768454670906067\n",
      "Epoch: 0, Loss:  0.6454991102218628\n",
      "Epoch: 0, Loss:  0.7172513604164124\n",
      "Epoch: 0, Loss:  0.5613224506378174\n",
      "Epoch: 1, Loss:  0.6724573373794556\n",
      "Epoch: 1, Loss:  0.6225578784942627\n",
      "Epoch: 1, Loss:  0.6318570971488953\n",
      "Epoch: 1, Loss:  0.647365927696228\n",
      "Epoch: 1, Loss:  0.5562658309936523\n",
      "Epoch: 1, Loss:  0.6702675819396973\n",
      "Epoch: 1, Loss:  0.649742841720581\n",
      "Epoch: 1, Loss:  0.6850372552871704\n",
      "Epoch: 1, Loss:  0.708747148513794\n",
      "Epoch: 1, Loss:  0.6954728364944458\n",
      "Epoch: 1, Loss:  0.6926897168159485\n",
      "Epoch: 1, Loss:  0.6611490249633789\n",
      "Epoch: 1, Loss:  0.5999903678894043\n",
      "Epoch: 1, Loss:  0.6548845767974854\n",
      "Epoch: 1, Loss:  0.5743085145950317\n",
      "Epoch: 1, Loss:  0.5682060122489929\n",
      "Epoch: 1, Loss:  0.574742317199707\n",
      "Epoch: 1, Loss:  0.5515488386154175\n",
      "Epoch: 1, Loss:  0.5989580154418945\n",
      "Epoch: 1, Loss:  0.5513836145401001\n",
      "Epoch: 1, Loss:  0.5645912289619446\n",
      "Epoch: 1, Loss:  0.5761138200759888\n",
      "Epoch: 1, Loss:  0.4866061210632324\n",
      "Epoch: 1, Loss:  0.6396450400352478\n",
      "Epoch: 1, Loss:  0.6767210960388184\n",
      "Epoch: 1, Loss:  0.6702706217765808\n",
      "Epoch: 1, Loss:  0.5403351783752441\n",
      "Epoch: 1, Loss:  0.6055344343185425\n",
      "Epoch: 1, Loss:  0.5849673748016357\n",
      "Epoch: 1, Loss:  0.6624826788902283\n",
      "Epoch: 1, Loss:  0.6010919213294983\n",
      "Epoch: 1, Loss:  0.7234202027320862\n",
      "Epoch: 1, Loss:  0.6615465879440308\n",
      "Epoch: 1, Loss:  0.6808027029037476\n",
      "Epoch: 1, Loss:  0.6696380376815796\n",
      "Epoch: 1, Loss:  0.6510475873947144\n",
      "Epoch: 1, Loss:  0.5710464119911194\n",
      "Epoch: 1, Loss:  0.5992334485054016\n",
      "Epoch: 1, Loss:  0.5418508648872375\n",
      "Epoch: 2, Loss:  0.6324418187141418\n",
      "Epoch: 2, Loss:  0.5083619952201843\n",
      "Epoch: 2, Loss:  0.5509639978408813\n",
      "Epoch: 2, Loss:  0.5981742739677429\n",
      "Epoch: 2, Loss:  0.6286059617996216\n",
      "Epoch: 2, Loss:  0.7060484886169434\n",
      "Epoch: 2, Loss:  0.8159942030906677\n",
      "Epoch: 2, Loss:  0.47983917593955994\n",
      "Epoch: 2, Loss:  0.6609704494476318\n",
      "Epoch: 2, Loss:  0.575007975101471\n",
      "Epoch: 2, Loss:  0.5991003513336182\n",
      "Epoch: 2, Loss:  0.6223201751708984\n",
      "Epoch: 2, Loss:  0.7616448402404785\n",
      "Epoch: 2, Loss:  0.57466721534729\n",
      "Epoch: 2, Loss:  0.5748660564422607\n",
      "Epoch: 2, Loss:  0.6458779573440552\n",
      "Epoch: 2, Loss:  0.6459413766860962\n",
      "Epoch: 2, Loss:  0.5748827457427979\n",
      "Epoch: 2, Loss:  0.6471712589263916\n",
      "Epoch: 2, Loss:  0.648247241973877\n",
      "Epoch: 2, Loss:  0.5276505351066589\n",
      "Epoch: 2, Loss:  0.6996752023696899\n",
      "Epoch: 2, Loss:  0.5747144818305969\n",
      "Epoch: 2, Loss:  0.5650477409362793\n",
      "Epoch: 2, Loss:  0.6349576711654663\n",
      "Epoch: 2, Loss:  0.70835280418396\n",
      "Epoch: 2, Loss:  0.6844490766525269\n",
      "Epoch: 2, Loss:  0.5510814785957336\n",
      "Epoch: 2, Loss:  0.6607496738433838\n",
      "Epoch: 2, Loss:  0.623602032661438\n",
      "Epoch: 2, Loss:  0.6379188299179077\n",
      "Epoch: 2, Loss:  0.6377969980239868\n",
      "Epoch: 2, Loss:  0.5106538534164429\n",
      "Epoch: 2, Loss:  0.646077036857605\n",
      "Epoch: 2, Loss:  0.6695798635482788\n",
      "Epoch: 2, Loss:  0.5750395059585571\n",
      "Epoch: 2, Loss:  0.6695067286491394\n",
      "Epoch: 2, Loss:  0.6457955837249756\n",
      "Epoch: 2, Loss:  0.6173807978630066\n",
      "Epoch: 3, Loss:  0.5750841498374939\n",
      "Epoch: 3, Loss:  0.6425561904907227\n",
      "Epoch: 3, Loss:  0.6753455400466919\n",
      "Epoch: 3, Loss:  0.7074099779129028\n",
      "Epoch: 3, Loss:  0.5983129739761353\n",
      "Epoch: 3, Loss:  0.6116129159927368\n",
      "Epoch: 3, Loss:  0.5955300331115723\n",
      "Epoch: 3, Loss:  0.5208740234375\n",
      "Epoch: 3, Loss:  0.5227806568145752\n",
      "Epoch: 3, Loss:  0.5772038698196411\n",
      "Epoch: 3, Loss:  0.547257661819458\n",
      "Epoch: 3, Loss:  0.5510253310203552\n",
      "Epoch: 3, Loss:  0.6225075721740723\n",
      "Epoch: 3, Loss:  0.45600682497024536\n",
      "Epoch: 3, Loss:  0.6104351282119751\n",
      "Epoch: 3, Loss:  0.6220024228096008\n",
      "Epoch: 3, Loss:  0.5271590948104858\n",
      "Epoch: 3, Loss:  0.4558800160884857\n",
      "Epoch: 3, Loss:  0.5216981172561646\n",
      "Epoch: 3, Loss:  0.5985104441642761\n",
      "Epoch: 3, Loss:  0.6230864524841309\n",
      "Epoch: 3, Loss:  0.7116214036941528\n",
      "Epoch: 3, Loss:  0.5989359021186829\n",
      "Epoch: 3, Loss:  0.6786571741104126\n",
      "Epoch: 3, Loss:  0.6345607042312622\n",
      "Epoch: 3, Loss:  0.5510261058807373\n",
      "Epoch: 3, Loss:  0.6637017130851746\n",
      "Epoch: 3, Loss:  0.5034407377243042\n",
      "Epoch: 3, Loss:  0.598286509513855\n",
      "Epoch: 3, Loss:  0.6162451505661011\n",
      "Epoch: 3, Loss:  0.5900157690048218\n",
      "Epoch: 3, Loss:  0.5747810006141663\n",
      "Epoch: 3, Loss:  0.5734769105911255\n",
      "Epoch: 3, Loss:  0.5744799375534058\n",
      "Epoch: 3, Loss:  0.5507400035858154\n",
      "Epoch: 3, Loss:  0.66610187292099\n",
      "Epoch: 3, Loss:  0.6220407485961914\n",
      "Epoch: 3, Loss:  0.5745948553085327\n",
      "Epoch: 3, Loss:  0.5797865986824036\n",
      "Epoch: 4, Loss:  0.5743748545646667\n",
      "Epoch: 4, Loss:  0.5497403740882874\n",
      "Epoch: 4, Loss:  0.552810549736023\n",
      "Epoch: 4, Loss:  0.5508143901824951\n",
      "Epoch: 4, Loss:  0.522099494934082\n",
      "Epoch: 4, Loss:  0.6269429922103882\n",
      "Epoch: 4, Loss:  0.6438705325126648\n",
      "Epoch: 4, Loss:  0.6993162631988525\n",
      "Epoch: 4, Loss:  0.5886260867118835\n",
      "Epoch: 4, Loss:  0.5508144497871399\n",
      "Epoch: 4, Loss:  0.6220771670341492\n",
      "Epoch: 4, Loss:  0.6355526447296143\n",
      "Epoch: 4, Loss:  0.6220077276229858\n",
      "Epoch: 4, Loss:  0.5745736360549927\n",
      "Epoch: 4, Loss:  0.6607654094696045\n",
      "Epoch: 4, Loss:  0.598307728767395\n",
      "Epoch: 4, Loss:  0.6607438325881958\n",
      "Epoch: 4, Loss:  0.5751566886901855\n",
      "Epoch: 4, Loss:  0.645385205745697\n",
      "Epoch: 4, Loss:  0.6694775819778442\n",
      "Epoch: 4, Loss:  0.5371169447898865\n",
      "Epoch: 4, Loss:  0.529592752456665\n",
      "Epoch: 4, Loss:  0.5744678974151611\n",
      "Epoch: 4, Loss:  0.6113505363464355\n",
      "Epoch: 4, Loss:  0.6369794607162476\n",
      "Epoch: 4, Loss:  0.6132327318191528\n",
      "Epoch: 4, Loss:  0.6607251763343811\n",
      "Epoch: 4, Loss:  0.5038170218467712\n",
      "Epoch: 4, Loss:  0.5032894611358643\n",
      "Epoch: 4, Loss:  0.5270119905471802\n",
      "Epoch: 4, Loss:  0.6845036745071411\n",
      "Epoch: 4, Loss:  0.642675518989563\n",
      "Epoch: 4, Loss:  0.6220908164978027\n",
      "Epoch: 4, Loss:  0.6844282150268555\n",
      "Epoch: 4, Loss:  0.5981297492980957\n",
      "Epoch: 4, Loss:  0.5032742619514465\n",
      "Epoch: 4, Loss:  0.5525252819061279\n",
      "Epoch: 4, Loss:  0.6134853363037109\n",
      "Epoch: 4, Loss:  0.6412272453308105\n",
      "Epoch: 5, Loss:  0.6748061180114746\n",
      "Epoch: 5, Loss:  0.5123878717422485\n",
      "Epoch: 5, Loss:  0.574603796005249\n",
      "Epoch: 5, Loss:  0.582027792930603\n",
      "Epoch: 5, Loss:  0.6219325065612793\n",
      "Epoch: 5, Loss:  0.5895195007324219\n",
      "Epoch: 5, Loss:  0.7227520942687988\n",
      "Epoch: 5, Loss:  0.527157187461853\n",
      "Epoch: 5, Loss:  0.7081871628761292\n",
      "Epoch: 5, Loss:  0.5745274424552917\n",
      "Epoch: 5, Loss:  0.6694492101669312\n",
      "Epoch: 5, Loss:  0.5981811285018921\n",
      "Epoch: 5, Loss:  0.6283460855484009\n",
      "Epoch: 5, Loss:  0.5531424880027771\n",
      "Epoch: 5, Loss:  0.6032472252845764\n",
      "Epoch: 5, Loss:  0.6222962141036987\n",
      "Epoch: 5, Loss:  0.6848297119140625\n",
      "Epoch: 5, Loss:  0.6668746471405029\n",
      "Epoch: 5, Loss:  0.6432653665542603\n",
      "Epoch: 5, Loss:  0.5770969390869141\n",
      "Epoch: 5, Loss:  0.6131593585014343\n",
      "Epoch: 5, Loss:  0.5032471418380737\n",
      "Epoch: 5, Loss:  0.5750638842582703\n",
      "Epoch: 5, Loss:  0.6131775379180908\n",
      "Epoch: 5, Loss:  0.660268247127533\n",
      "Epoch: 5, Loss:  0.6660781502723694\n",
      "Epoch: 5, Loss:  0.6281700730323792\n",
      "Epoch: 5, Loss:  0.5981659889221191\n",
      "Epoch: 5, Loss:  0.5269824266433716\n",
      "Epoch: 5, Loss:  0.5748094916343689\n",
      "Epoch: 5, Loss:  0.6520050168037415\n",
      "Epoch: 5, Loss:  0.6282384991645813\n",
      "Epoch: 5, Loss:  0.6843795776367188\n",
      "Epoch: 5, Loss:  0.5982431173324585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss:  0.5524152517318726\n",
      "Epoch: 5, Loss:  0.623091459274292\n",
      "Epoch: 5, Loss:  0.5507504940032959\n",
      "Epoch: 5, Loss:  0.5300461053848267\n",
      "Epoch: 5, Loss:  0.6408995985984802\n",
      "Epoch: 6, Loss:  0.5301190614700317\n",
      "Epoch: 6, Loss:  0.5032708644866943\n",
      "Epoch: 6, Loss:  0.621981143951416\n",
      "Epoch: 6, Loss:  0.5894856452941895\n",
      "Epoch: 6, Loss:  0.6494957804679871\n",
      "Epoch: 6, Loss:  0.6938496232032776\n",
      "Epoch: 6, Loss:  0.6457271575927734\n",
      "Epoch: 6, Loss:  0.6683382987976074\n",
      "Epoch: 6, Loss:  0.637005090713501\n",
      "Epoch: 6, Loss:  0.5507354140281677\n",
      "Epoch: 6, Loss:  0.589728593826294\n",
      "Epoch: 6, Loss:  0.698867917060852\n",
      "Epoch: 6, Loss:  0.5271650552749634\n",
      "Epoch: 6, Loss:  0.6585216522216797\n",
      "Epoch: 6, Loss:  0.5984983444213867\n",
      "Epoch: 6, Loss:  0.5926016569137573\n",
      "Epoch: 6, Loss:  0.613937497138977\n",
      "Epoch: 6, Loss:  0.5744919776916504\n",
      "Epoch: 6, Loss:  0.6132736206054688\n",
      "Epoch: 6, Loss:  0.6268442273139954\n",
      "Epoch: 6, Loss:  0.6136600971221924\n",
      "Epoch: 6, Loss:  0.7144628763198853\n",
      "Epoch: 6, Loss:  0.6607757806777954\n",
      "Epoch: 6, Loss:  0.6098379492759705\n",
      "Epoch: 6, Loss:  0.5894320011138916\n",
      "Epoch: 6, Loss:  0.599327802658081\n",
      "Epoch: 6, Loss:  0.6219930052757263\n",
      "Epoch: 6, Loss:  0.5971958041191101\n",
      "Epoch: 6, Loss:  0.5507304668426514\n",
      "Epoch: 6, Loss:  0.6008722186088562\n",
      "Epoch: 6, Loss:  0.5744880437850952\n",
      "Epoch: 6, Loss:  0.6606347560882568\n",
      "Epoch: 6, Loss:  0.6615659594535828\n",
      "Epoch: 6, Loss:  0.7156050205230713\n",
      "Epoch: 6, Loss:  0.5421168208122253\n",
      "Epoch: 6, Loss:  0.6994791626930237\n",
      "Epoch: 6, Loss:  0.621951699256897\n",
      "Epoch: 6, Loss:  0.5758804678916931\n",
      "Epoch: 6, Loss:  0.47518834471702576\n",
      "Epoch: 7, Loss:  0.5982815027236938\n",
      "Epoch: 7, Loss:  0.6457513570785522\n",
      "Epoch: 7, Loss:  0.6679295301437378\n",
      "Epoch: 7, Loss:  0.5622652173042297\n",
      "Epoch: 7, Loss:  0.5982303619384766\n",
      "Epoch: 7, Loss:  0.5026945471763611\n",
      "Epoch: 7, Loss:  0.6369152069091797\n",
      "Epoch: 7, Loss:  0.6219900250434875\n",
      "Epoch: 7, Loss:  0.5514474511146545\n",
      "Epoch: 7, Loss:  0.5657426714897156\n",
      "Epoch: 7, Loss:  0.5115083456039429\n",
      "Epoch: 7, Loss:  0.6132226586341858\n",
      "Epoch: 7, Loss:  0.574504017829895\n",
      "Epoch: 7, Loss:  0.5984669327735901\n",
      "Epoch: 7, Loss:  0.5607953071594238\n",
      "Epoch: 7, Loss:  0.6694299578666687\n",
      "Epoch: 7, Loss:  0.6413980722427368\n",
      "Epoch: 7, Loss:  0.5886155366897583\n",
      "Epoch: 7, Loss:  0.6297142505645752\n",
      "Epoch: 7, Loss:  0.5982112288475037\n",
      "Epoch: 7, Loss:  0.6369445323944092\n",
      "Epoch: 7, Loss:  0.5507326126098633\n",
      "Epoch: 7, Loss:  0.6607348918914795\n",
      "Epoch: 7, Loss:  0.6132137775421143\n",
      "Epoch: 7, Loss:  0.5744607448577881\n",
      "Epoch: 7, Loss:  0.5020294189453125\n",
      "Epoch: 7, Loss:  0.636968731880188\n",
      "Epoch: 7, Loss:  0.5271987915039062\n",
      "Epoch: 7, Loss:  0.5751833319664001\n",
      "Epoch: 7, Loss:  0.5657083988189697\n",
      "Epoch: 7, Loss:  0.5734239816665649\n",
      "Epoch: 7, Loss:  0.526984453201294\n",
      "Epoch: 7, Loss:  0.6330089569091797\n",
      "Epoch: 7, Loss:  0.5518125891685486\n",
      "Epoch: 7, Loss:  0.5268163681030273\n",
      "Epoch: 7, Loss:  0.5034000277519226\n",
      "Epoch: 7, Loss:  0.5538222789764404\n",
      "Epoch: 7, Loss:  0.5619571805000305\n",
      "Epoch: 7, Loss:  0.5790252685546875\n",
      "Epoch: 8, Loss:  0.5269318222999573\n",
      "Epoch: 8, Loss:  0.6220227479934692\n",
      "Epoch: 8, Loss:  0.5986089110374451\n",
      "Epoch: 8, Loss:  0.6607805490493774\n",
      "Epoch: 8, Loss:  0.5270062685012817\n",
      "Epoch: 8, Loss:  0.5805988311767578\n",
      "Epoch: 8, Loss:  0.5894817113876343\n",
      "Epoch: 8, Loss:  0.5745512843132019\n",
      "Epoch: 8, Loss:  0.5894861221313477\n",
      "Epoch: 8, Loss:  0.5036693811416626\n",
      "Epoch: 8, Loss:  0.5270059108734131\n",
      "Epoch: 8, Loss:  0.6220248937606812\n",
      "Epoch: 8, Loss:  0.5269750952720642\n",
      "Epoch: 8, Loss:  0.598228931427002\n",
      "Epoch: 8, Loss:  0.5512853860855103\n",
      "Epoch: 8, Loss:  0.6219399571418762\n",
      "Epoch: 8, Loss:  0.6213376522064209\n",
      "Epoch: 8, Loss:  0.5560688376426697\n",
      "Epoch: 8, Loss:  0.5744837522506714\n",
      "Epoch: 8, Loss:  0.4557848572731018\n",
      "Epoch: 8, Loss:  0.5304727554321289\n",
      "Epoch: 8, Loss:  0.52696692943573\n",
      "Epoch: 8, Loss:  0.5314077138900757\n",
      "Epoch: 8, Loss:  0.5507436990737915\n",
      "Epoch: 8, Loss:  0.565747857093811\n",
      "Epoch: 8, Loss:  0.6456847190856934\n",
      "Epoch: 8, Loss:  0.550734281539917\n",
      "Epoch: 8, Loss:  0.6986632347106934\n",
      "Epoch: 8, Loss:  0.5744723081588745\n",
      "Epoch: 8, Loss:  0.526950478553772\n",
      "Epoch: 8, Loss:  0.56573885679245\n",
      "Epoch: 8, Loss:  0.6694902777671814\n",
      "Epoch: 8, Loss:  0.6220294833183289\n",
      "Epoch: 8, Loss:  0.6603202819824219\n",
      "Epoch: 8, Loss:  0.6457110643386841\n",
      "Epoch: 8, Loss:  0.6519642472267151\n",
      "Epoch: 8, Loss:  0.5744563341140747\n",
      "Epoch: 8, Loss:  0.6051892638206482\n",
      "Epoch: 8, Loss:  0.5033186674118042\n",
      "Epoch: 9, Loss:  0.5507183074951172\n",
      "Epoch: 9, Loss:  0.7107685804367065\n",
      "Epoch: 9, Loss:  0.5982391834259033\n",
      "Epoch: 9, Loss:  0.5984798669815063\n",
      "Epoch: 9, Loss:  0.5744553804397583\n",
      "Epoch: 9, Loss:  0.5507112145423889\n",
      "Epoch: 9, Loss:  0.5746568441390991\n",
      "Epoch: 9, Loss:  0.5657216310501099\n",
      "Epoch: 9, Loss:  0.5753404498100281\n",
      "Epoch: 9, Loss:  0.584351658821106\n",
      "Epoch: 9, Loss:  0.6600875854492188\n",
      "Epoch: 9, Loss:  0.6134514212608337\n",
      "Epoch: 9, Loss:  0.6098254919052124\n",
      "Epoch: 9, Loss:  0.5269609689712524\n",
      "Epoch: 9, Loss:  0.5982006788253784\n",
      "Epoch: 9, Loss:  0.6607009172439575\n",
      "Epoch: 9, Loss:  0.5988082885742188\n",
      "Epoch: 9, Loss:  0.6844381093978882\n",
      "Epoch: 9, Loss:  0.5268819332122803\n",
      "Epoch: 9, Loss:  0.6610947251319885\n",
      "Epoch: 9, Loss:  0.5057882070541382\n",
      "Epoch: 9, Loss:  0.4795171022415161\n",
      "Epoch: 9, Loss:  0.6276072263717651\n",
      "Epoch: 9, Loss:  0.5270041227340698\n",
      "Epoch: 9, Loss:  0.5744718909263611\n",
      "Epoch: 9, Loss:  0.6931483745574951\n",
      "Epoch: 9, Loss:  0.5507922172546387\n",
      "Epoch: 9, Loss:  0.5032288432121277\n",
      "Epoch: 9, Loss:  0.6365622282028198\n",
      "Epoch: 9, Loss:  0.6132686734199524\n",
      "Epoch: 9, Loss:  0.5981956720352173\n",
      "Epoch: 9, Loss:  0.5507177114486694\n",
      "Epoch: 9, Loss:  0.616950511932373\n",
      "Epoch: 9, Loss:  0.5507088899612427\n",
      "Epoch: 9, Loss:  0.6132024526596069\n",
      "Epoch: 9, Loss:  0.49450570344924927\n",
      "Epoch: 9, Loss:  0.5514630079269409\n",
      "Epoch: 9, Loss:  0.5745466947555542\n",
      "Epoch: 9, Loss:  0.46524935960769653\n"
     ]
    }
   ],
   "source": [
    "#for training\n",
    "torch.cuda.empty_cache()\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)\n",
    "\n",
    "\n",
    "#Save PATH\n",
    "PATH = r\"C:\\Users\\jethr\\OneDrive\\Documents\\Kaggle\\NLPTweets\\model.pt\"\n",
    "\n",
    "# for loading Saved model\n",
    "#model = torch.load(PATH)\n",
    "#model.load_state_dict(torch.load(PATH))\n",
    "# Save trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor epoch in range(EPOCHS):\\n    outputs, targets = validation(epoch)\\n    outputs = np.array(outputs) >= 0.5\\n    accuracy = metrics.accuracy_score(targets, outputs)\\n    f1_score_micro = metrics.f1_score(targets, outputs, average=\\'micro\\')\\n    f1_score_macro = metrics.f1_score(targets, outputs, average=\\'macro\\')\\n    print(f\"Accuracy Score = {accuracy}\")\\n    print(f\"F1 Score (Micro) = {f1_score_micro}\")\\n    print(f\"F1 Score (Macro) = {f1_score_macro}\")\\n'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for epoch in range(EPOCHS):\n",
    "    outputs, targets = validation(epoch)\n",
    "    outputs = np.array(outputs) >= 0.5\n",
    "    accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "    print(f\"Accuracy Score = {accuracy}\")\n",
    "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "    print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "l1.embeddings.position_ids \t torch.Size([1, 512])\n",
      "l1.embeddings.word_embeddings.weight \t torch.Size([30522, 768])\n",
      "l1.embeddings.position_embeddings.weight \t torch.Size([512, 768])\n",
      "l1.embeddings.token_type_embeddings.weight \t torch.Size([2, 768])\n",
      "l1.embeddings.LayerNorm.weight \t torch.Size([768])\n",
      "l1.embeddings.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.0.attention.self.query.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.0.attention.self.query.bias \t torch.Size([768])\n",
      "l1.encoder.layer.0.attention.self.key.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.0.attention.self.key.bias \t torch.Size([768])\n",
      "l1.encoder.layer.0.attention.self.value.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.0.attention.self.value.bias \t torch.Size([768])\n",
      "l1.encoder.layer.0.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.0.attention.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.0.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.0.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.0.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "l1.encoder.layer.0.intermediate.dense.bias \t torch.Size([3072])\n",
      "l1.encoder.layer.0.output.dense.weight \t torch.Size([768, 3072])\n",
      "l1.encoder.layer.0.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.0.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.0.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.1.attention.self.query.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.1.attention.self.query.bias \t torch.Size([768])\n",
      "l1.encoder.layer.1.attention.self.key.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.1.attention.self.key.bias \t torch.Size([768])\n",
      "l1.encoder.layer.1.attention.self.value.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.1.attention.self.value.bias \t torch.Size([768])\n",
      "l1.encoder.layer.1.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.1.attention.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.1.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.1.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.1.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "l1.encoder.layer.1.intermediate.dense.bias \t torch.Size([3072])\n",
      "l1.encoder.layer.1.output.dense.weight \t torch.Size([768, 3072])\n",
      "l1.encoder.layer.1.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.1.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.1.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.2.attention.self.query.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.2.attention.self.query.bias \t torch.Size([768])\n",
      "l1.encoder.layer.2.attention.self.key.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.2.attention.self.key.bias \t torch.Size([768])\n",
      "l1.encoder.layer.2.attention.self.value.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.2.attention.self.value.bias \t torch.Size([768])\n",
      "l1.encoder.layer.2.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.2.attention.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.2.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.2.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.2.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "l1.encoder.layer.2.intermediate.dense.bias \t torch.Size([3072])\n",
      "l1.encoder.layer.2.output.dense.weight \t torch.Size([768, 3072])\n",
      "l1.encoder.layer.2.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.2.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.2.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.3.attention.self.query.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.3.attention.self.query.bias \t torch.Size([768])\n",
      "l1.encoder.layer.3.attention.self.key.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.3.attention.self.key.bias \t torch.Size([768])\n",
      "l1.encoder.layer.3.attention.self.value.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.3.attention.self.value.bias \t torch.Size([768])\n",
      "l1.encoder.layer.3.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.3.attention.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.3.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.3.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.3.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "l1.encoder.layer.3.intermediate.dense.bias \t torch.Size([3072])\n",
      "l1.encoder.layer.3.output.dense.weight \t torch.Size([768, 3072])\n",
      "l1.encoder.layer.3.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.3.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.3.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.4.attention.self.query.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.4.attention.self.query.bias \t torch.Size([768])\n",
      "l1.encoder.layer.4.attention.self.key.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.4.attention.self.key.bias \t torch.Size([768])\n",
      "l1.encoder.layer.4.attention.self.value.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.4.attention.self.value.bias \t torch.Size([768])\n",
      "l1.encoder.layer.4.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.4.attention.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.4.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.4.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.4.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "l1.encoder.layer.4.intermediate.dense.bias \t torch.Size([3072])\n",
      "l1.encoder.layer.4.output.dense.weight \t torch.Size([768, 3072])\n",
      "l1.encoder.layer.4.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.4.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.4.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.5.attention.self.query.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.5.attention.self.query.bias \t torch.Size([768])\n",
      "l1.encoder.layer.5.attention.self.key.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.5.attention.self.key.bias \t torch.Size([768])\n",
      "l1.encoder.layer.5.attention.self.value.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.5.attention.self.value.bias \t torch.Size([768])\n",
      "l1.encoder.layer.5.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.5.attention.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.5.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.5.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.5.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "l1.encoder.layer.5.intermediate.dense.bias \t torch.Size([3072])\n",
      "l1.encoder.layer.5.output.dense.weight \t torch.Size([768, 3072])\n",
      "l1.encoder.layer.5.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.5.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.5.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.6.attention.self.query.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.6.attention.self.query.bias \t torch.Size([768])\n",
      "l1.encoder.layer.6.attention.self.key.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.6.attention.self.key.bias \t torch.Size([768])\n",
      "l1.encoder.layer.6.attention.self.value.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.6.attention.self.value.bias \t torch.Size([768])\n",
      "l1.encoder.layer.6.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.6.attention.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.6.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.6.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.6.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "l1.encoder.layer.6.intermediate.dense.bias \t torch.Size([3072])\n",
      "l1.encoder.layer.6.output.dense.weight \t torch.Size([768, 3072])\n",
      "l1.encoder.layer.6.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.6.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.6.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.7.attention.self.query.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.7.attention.self.query.bias \t torch.Size([768])\n",
      "l1.encoder.layer.7.attention.self.key.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.7.attention.self.key.bias \t torch.Size([768])\n",
      "l1.encoder.layer.7.attention.self.value.weight \t torch.Size([768, 768])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1.encoder.layer.7.attention.self.value.bias \t torch.Size([768])\n",
      "l1.encoder.layer.7.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.7.attention.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.7.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.7.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.7.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "l1.encoder.layer.7.intermediate.dense.bias \t torch.Size([3072])\n",
      "l1.encoder.layer.7.output.dense.weight \t torch.Size([768, 3072])\n",
      "l1.encoder.layer.7.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.7.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.7.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.8.attention.self.query.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.8.attention.self.query.bias \t torch.Size([768])\n",
      "l1.encoder.layer.8.attention.self.key.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.8.attention.self.key.bias \t torch.Size([768])\n",
      "l1.encoder.layer.8.attention.self.value.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.8.attention.self.value.bias \t torch.Size([768])\n",
      "l1.encoder.layer.8.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.8.attention.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.8.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.8.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.8.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "l1.encoder.layer.8.intermediate.dense.bias \t torch.Size([3072])\n",
      "l1.encoder.layer.8.output.dense.weight \t torch.Size([768, 3072])\n",
      "l1.encoder.layer.8.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.8.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.8.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.9.attention.self.query.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.9.attention.self.query.bias \t torch.Size([768])\n",
      "l1.encoder.layer.9.attention.self.key.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.9.attention.self.key.bias \t torch.Size([768])\n",
      "l1.encoder.layer.9.attention.self.value.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.9.attention.self.value.bias \t torch.Size([768])\n",
      "l1.encoder.layer.9.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.9.attention.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.9.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.9.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.9.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "l1.encoder.layer.9.intermediate.dense.bias \t torch.Size([3072])\n",
      "l1.encoder.layer.9.output.dense.weight \t torch.Size([768, 3072])\n",
      "l1.encoder.layer.9.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.9.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.9.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.10.attention.self.query.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.10.attention.self.query.bias \t torch.Size([768])\n",
      "l1.encoder.layer.10.attention.self.key.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.10.attention.self.key.bias \t torch.Size([768])\n",
      "l1.encoder.layer.10.attention.self.value.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.10.attention.self.value.bias \t torch.Size([768])\n",
      "l1.encoder.layer.10.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.10.attention.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.10.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.10.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.10.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "l1.encoder.layer.10.intermediate.dense.bias \t torch.Size([3072])\n",
      "l1.encoder.layer.10.output.dense.weight \t torch.Size([768, 3072])\n",
      "l1.encoder.layer.10.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.10.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.10.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.11.attention.self.query.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.11.attention.self.query.bias \t torch.Size([768])\n",
      "l1.encoder.layer.11.attention.self.key.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.11.attention.self.key.bias \t torch.Size([768])\n",
      "l1.encoder.layer.11.attention.self.value.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.11.attention.self.value.bias \t torch.Size([768])\n",
      "l1.encoder.layer.11.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "l1.encoder.layer.11.attention.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.11.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.11.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.encoder.layer.11.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "l1.encoder.layer.11.intermediate.dense.bias \t torch.Size([3072])\n",
      "l1.encoder.layer.11.output.dense.weight \t torch.Size([768, 3072])\n",
      "l1.encoder.layer.11.output.dense.bias \t torch.Size([768])\n",
      "l1.encoder.layer.11.output.LayerNorm.weight \t torch.Size([768])\n",
      "l1.encoder.layer.11.output.LayerNorm.bias \t torch.Size([768])\n",
      "l1.pooler.dense.weight \t torch.Size([768, 768])\n",
      "l1.pooler.dense.bias \t torch.Size([768])\n",
      "l3.weight \t torch.Size([30, 768])\n",
      "l3.bias \t torch.Size([30])\n",
      "l4.weight \t torch.Size([1, 30])\n",
      "l4.bias \t torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction\n",
    "def predict(test_dataset,tokenizer,max_len,model):\n",
    "    tweet = str(test_dataset[\"tweet\"])\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        tweet,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        pad_to_max_length=True,\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "    \n",
    "    #encode prediction dataset\n",
    "    predict_ids = torch.tensor(inputs['input_ids'],dtype=torch.long)\n",
    "    predict_mask = torch.tensor(inputs['attention_mask'],dtype=torch.long)\n",
    "    predict_token_type_ids = torch.tensor(inputs[\"token_type_ids\"],dtype=torch.long)\n",
    "    \n",
    "    print(predict_ids.shape)\n",
    "    \n",
    "    #forward pass\n",
    "    outputs = model(predict_ids, predict_mask, predict_token_type_ids)\n",
    "    \n",
    "    return outputs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = CustomDataset(df_test, tokenizer, MAX_LEN,\"test\")\n",
    "eval_params = {'batch_size': 4,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "eval_loader = DataLoader(eval_data,**eval_params)\n",
    "model.eval()\n",
    "results = []\n",
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    for _,data in enumerate(eval_loader,0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)  \n",
    "            outputs = model(ids, mask, token_type_ids).tolist()\n",
    "            results.append(outputs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_predictions = [item for sublist in results for item in sublist]\n",
    "df_predict = pd.DataFrame(flat_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: ylabel='Count'>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check outputs\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.histplot(df_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "3258    1\n",
       "3259    1\n",
       "3260    1\n",
       "3261    0\n",
       "3262    1\n",
       "Name: 0, Length: 3263, dtype: int32"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set threshold\n",
    "df_predict[df_predict>=0.5] = int(1)\n",
    "df_predict[df_predict< 0.5 ] = int(0)\n",
    "df_predict[0].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        int64\n",
       "target    int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating submission data.\n",
    "\n",
    "submission = pd.read_csv(r\"C:\\Users\\jethr\\OneDrive\\Documents\\Kaggle\\NLPTweets\\sample_submission.csv\")\n",
    "submission['target'] = df_predict[0].astype('int')\n",
    "submission.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving submission to '.csv' file:\n",
    "\n",
    "submission.to_csv(r\"C:\\Users\\jethr\\OneDrive\\Documents\\Kaggle\\NLPTweets\\sample_submission2.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
